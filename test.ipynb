{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sonar'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 130\u001b[0m\n\u001b[1;32m    120\u001b[0m     dataloader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(\n\u001b[1;32m    121\u001b[0m         streaming_dataset,\n\u001b[1;32m    122\u001b[0m         batch_size\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m64\u001b[39m),\n\u001b[1;32m    123\u001b[0m         num_workers\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mnum_workers\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m4\u001b[39m),\n\u001b[1;32m    124\u001b[0m         pin_memory\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     )\n\u001b[1;32m    127\u001b[0m     \u001b[39mreturn\u001b[39;00m dataloader\n\u001b[0;32m--> 130\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msonar\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minference_pipelines\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m TextToEmbeddingModelPipeline\n\u001b[1;32m    131\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\n\u001b[1;32m    133\u001b[0m t2vec_model \u001b[39m=\u001b[39m TextToEmbeddingModelPipeline(\n\u001b[1;32m    134\u001b[0m     encoder\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtext_sonar_basic_encoder\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    135\u001b[0m     tokenizer\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtext_sonar_basic_encoder\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    136\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sonar'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import logging\n",
    "import json\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import (\n",
    "    ShardingStrategy\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "class StreamingCC12MDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        dataset, \n",
    "        siglip_processor, \n",
    "        sonar_text_encoder,\n",
    "        max_samples=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Streaming dataset wrapper\n",
    "        \n",
    "        Args:\n",
    "            dataset: Hugging Face streaming dataset\n",
    "            siglip_processor: SIG-LIP image processor\n",
    "            sonar_text_encoder: SONAR text embedding model\n",
    "            max_samples: Optional limit on number of samples\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.siglip_processor = siglip_processor\n",
    "        self.sonar_text_encoder = sonar_text_encoder\n",
    "        self.max_samples = max_samples\n",
    "        \n",
    "        # Iterator for streaming\n",
    "        self.dataset_iter = iter(self.dataset)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Efficient iterator for streaming dataset\n",
    "        \"\"\"\n",
    "        # Track sample count\n",
    "        sample_count = 0\n",
    "        \n",
    "        for item in self.dataset_iter:\n",
    "            # Optional sample limit\n",
    "            if self.max_samples and sample_count >= self.max_samples:\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                # Process image\n",
    "                image = Image.open(io.BytesIO(item['jpg'])).convert('RGB')\n",
    "                \n",
    "                # SIG-LIP image processing\n",
    "                image_inputs = self.siglip_processor(\n",
    "                    images=[image], \n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                \n",
    "                # Extract visual embedding\n",
    "                with torch.no_grad():\n",
    "                    visual_embedding = model.get_image_features(**image_inputs)\n",
    "                \n",
    "                # Extract text \n",
    "                text = item['txt'].decode('utf-8')\n",
    "                \n",
    "                # SONAR text embedding\n",
    "                concept_embedding = self.sonar_text_encoder.predict(\n",
    "                    [text], \n",
    "                    source_lang=\"eng_Latn\"\n",
    "                )\n",
    "                \n",
    "                yield (\n",
    "                    visual_embedding.squeeze(), \n",
    "                    torch.tensor(concept_embedding).squeeze()\n",
    "                )\n",
    "                \n",
    "                sample_count += 1\n",
    "            \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing item: {e}\")\n",
    "                continue\n",
    "\n",
    "def create_streaming_dataloader(\n",
    "    dataset, \n",
    "    siglip_processor, \n",
    "    sonar_text_encoder,\n",
    "    config\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a dataloader for streaming dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset: Streaming Hugging Face dataset\n",
    "        siglip_processor: SIG-LIP image processor\n",
    "        sonar_text_encoder: SONAR text embedding model\n",
    "        config: Training configuration\n",
    "    \n",
    "    Returns:\n",
    "        DataLoader for streaming dataset\n",
    "    \"\"\"\n",
    "    # Create streaming dataset wrapper\n",
    "    streaming_dataset = StreamingCC12MDataset(\n",
    "        dataset,\n",
    "        siglip_processor=siglip_processor,\n",
    "        sonar_text_encoder=sonar_text_encoder,\n",
    "        max_samples=config.get('max_samples')\n",
    "    )\n",
    "    \n",
    "    # Create dataloader\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        streaming_dataset,\n",
    "        batch_size=config.get('batch_size', 64),\n",
    "        num_workers=config.get('num_workers', 4),\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "\n",
    "from sonar.inference_pipelines.text import TextToEmbeddingModelPipeline\n",
    "import transformers\n",
    "\n",
    "t2vec_model = TextToEmbeddingModelPipeline(\n",
    "    encoder=\"text_sonar_basic_encoder\",\n",
    "    tokenizer=\"text_sonar_basic_encoder\"\n",
    ")\n",
    "\n",
    "ckpt = \"google/siglip2-so400m-patch14-384\"\n",
    "siglip_model = transformers.AutoModel.from_pretrained(\n",
    "    ckpt, device_map=\"auto\"\n",
    ").eval()\n",
    "processor = transformers.AutoProcessor.from_pretrained(ckpt)\n",
    "        \n",
    "# In main training script\n",
    "dataset = load_dataset(\n",
    "    \"pixparse/cc12m-wds\",\n",
    "    cache_dir='/gpfs/gibbs/project/hartley/tjb76/Sig-CIP/Datasets',\n",
    "    streaming=True,\n",
    "    split='train'\n",
    ")\n",
    "\n",
    "train_loader = create_streaming_dataloader(\n",
    "    dataset,\n",
    "    siglip_processor=processor,\n",
    "    sonar_text_encoder=t2vec_model,\n",
    "    config=training_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-08 13:34:58.053572: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-08 13:34:58.067072: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744133698.082177 3122860 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744133698.086667 3122860 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-08 13:34:58.102425: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GemmaTokenizer'. \n",
      "The class this function is called from is 'SiglipTokenizer'.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m ckpt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgoogle/siglip2-so400m-patch14-384\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m siglip_model \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39mAutoModel\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      4\u001b[0m     ckpt, device_map\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m )\u001b[39m.\u001b[39meval()\n\u001b[0;32m----> 6\u001b[0m processor \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39;49mAutoProcessor\u001b[39m.\u001b[39;49mfrom_pretrained(ckpt)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/auto/processing_auto.py:334\u001b[0m, in \u001b[0;36mAutoProcessor.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[39mreturn\u001b[39;00m processor_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    331\u001b[0m         pretrained_model_name_or_path, trust_remote_code\u001b[39m=\u001b[39mtrust_remote_code, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    332\u001b[0m     )\n\u001b[1;32m    333\u001b[0m \u001b[39melif\u001b[39;00m processor_class \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m     \u001b[39mreturn\u001b[39;00m processor_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    335\u001b[0m         pretrained_model_name_or_path, trust_remote_code\u001b[39m=\u001b[39;49mtrust_remote_code, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    336\u001b[0m     )\n\u001b[1;32m    337\u001b[0m \u001b[39m# Last try: we use the PROCESSOR_MAPPING.\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m PROCESSOR_MAPPING:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/processing_utils.py:1070\u001b[0m, in \u001b[0;36mProcessorMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m   1067\u001b[0m \u001b[39mif\u001b[39;00m token \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1068\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mtoken\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m token\n\u001b[0;32m-> 1070\u001b[0m args \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_arguments_from_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1071\u001b[0m processor_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mget_processor_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1073\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mfrom_args_and_dict(args, processor_dict, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/processing_utils.py:1116\u001b[0m, in \u001b[0;36mProcessorMixin._get_arguments_from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1114\u001b[0m         attribute_class \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(transformers_module, class_name)\n\u001b[0;32m-> 1116\u001b[0m     args\u001b[39m.\u001b[39mappend(attribute_class\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[1;32m   1117\u001b[0m \u001b[39mreturn\u001b[39;00m args\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2052\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2049\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2050\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading file \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m from cache at \u001b[39m\u001b[39m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2052\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_from_pretrained(\n\u001b[1;32m   2053\u001b[0m     resolved_vocab_files,\n\u001b[1;32m   2054\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m   2055\u001b[0m     init_configuration,\n\u001b[1;32m   2056\u001b[0m     \u001b[39m*\u001b[39;49minit_inputs,\n\u001b[1;32m   2057\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   2058\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   2059\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   2060\u001b[0m     _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m   2061\u001b[0m     _is_local\u001b[39m=\u001b[39;49mis_local,\n\u001b[1;32m   2062\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49mtrust_remote_code,\n\u001b[1;32m   2063\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2064\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2292\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2290\u001b[0m \u001b[39m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2291\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2292\u001b[0m     tokenizer \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49minit_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minit_kwargs)\n\u001b[1;32m   2293\u001b[0m \u001b[39mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[1;32m   2294\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m   2295\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2296\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   2297\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/siglip/tokenization_siglip.py:123\u001b[0m, in \u001b[0;36mSiglipTokenizer.__init__\u001b[0;34m(self, vocab_file, eos_token, unk_token, pad_token, additional_special_tokens, sp_model_kwargs, model_max_length, do_lower_case, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_lower_case \u001b[39m=\u001b[39m do_lower_case\n\u001b[1;32m    121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_file \u001b[39m=\u001b[39m vocab_file\n\u001b[0;32m--> 123\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msp_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_spm_processor()\n\u001b[1;32m    124\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_file \u001b[39m=\u001b[39m vocab_file\n\u001b[1;32m    126\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[1;32m    127\u001b[0m     eos_token\u001b[39m=\u001b[39meos_token,\n\u001b[1;32m    128\u001b[0m     unk_token\u001b[39m=\u001b[39munk_token,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    135\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/siglip/tokenization_siglip.py:139\u001b[0m, in \u001b[0;36mSiglipTokenizer.get_spm_processor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_spm_processor\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    138\u001b[0m     tokenizer \u001b[39m=\u001b[39m spm\u001b[39m.\u001b[39mSentencePieceProcessor(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msp_model_kwargs)\n\u001b[0;32m--> 139\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocab_file, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    140\u001b[0m         sp_model \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\n\u001b[1;32m    141\u001b[0m         model_pb2 \u001b[39m=\u001b[39m import_protobuf()\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "ckpt = 'google/siglip2-so400m-patch14-384'\n",
    "siglip_model = transformers.AutoModel.from_pretrained(\n",
    "    ckpt, device_map=\"auto\"\n",
    ").eval()\n",
    "processor = transformers.AutoProcessor.from_pretrained(ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edce5fd2a37f453480552c19f96f1d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# load pipeline\n",
    "ckpt = \"google/siglip2-so400m-patch14-384\"\n",
    "image_classifier = pipeline(model=ckpt, task=\"zero-shot-image-classification\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SiglipVisionModel, SiglipImageProcessor, SiglipModel, AutoImageProcessor, AutoProcessor\n",
    "\n",
    "# Load only the vision components\n",
    "vision_model = SiglipVisionModel.from_pretrained(\"google/siglip2-base-patch16-224\", device_map=\"auto\").eval()\n",
    "image_processor = SiglipImageProcessor.from_pretrained(\"google/siglip2-base-patch16-224\")\n",
    "\n",
    "model = SiglipModel.from_pretrained(\"google/siglip2-base-patch16-224\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
